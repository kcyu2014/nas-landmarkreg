{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "home = os.environ['HOME']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "os.chdir(f'{home}/pycharm/automl')\n",
    "# os.chdir(f'{home}/pycharm/automl/search_policies/rnn')\n",
    "sys.path.append(f'{home}/pycharm/nasbench')\n",
    "sys.path.append(f'{home}/pycharm/automl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "seed = 1000\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.enabled = True\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.38076125 0.76041522 0.75035755]\n32\nCannot import graphviz package\n",
      "2 3\n[torch.Size([850, 1700]), torch.Size([850, 1700])]\n[torch.Size([850, 1700]), torch.Size([850, 1700])]\ntensor(0., grad_fn=<SumBackward0>)\ntensor(0., grad_fn=<SumBackward0>)\n",
      "torch.Size([14524, 64])\n",
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from search_policies.rnn.model import RNNModel\n",
    "from search_policies.rnn.softws.soft_weight_sharing_model import RNNModelSoftWS\n",
    "\n",
    "from genotypes import Genotype, PRIMITIVES\n",
    "from search_policies.search_space import SearchSpace\n",
    "from search_policies.search_configs import parser\n",
    "\n",
    "args = parser.parse_args(['--num_intermediate_node', '2']) \n",
    "search_space = SearchSpace(args)\n",
    "args.search_space = search_space\n",
    "search_space.genotype_from_id(0)\n",
    "\n",
    "args.softws_num_param_per_node = 4\n",
    "args.dropoute = 0\n",
    "args.dropouti = 0\n",
    "args.dropout = 0\n",
    "args.dropoutx = 0\n",
    "args.dropouth = 0\n",
    "\n",
    "ntoken = 10000\n",
    "genotype_id = 0\n",
    "batch_size = 64\n",
    "model = RNNModel(ntoken=ntoken, args=args, \n",
    "                 genotype_id=genotype_id, \n",
    "                 genotype=search_space.genotype_from_id(genotype_id, 2))\n",
    "\n",
    "ws_model = RNNModelSoftWS(\n",
    "    ntoken=ntoken, args=args, \n",
    "                 genotype_id=genotype_id, \n",
    "                 genotype=search_space.genotype_from_id(genotype_id, 2)\n",
    ")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def initialize_soft_ws_from_original(model, ws_model):\n",
    "    ws_model.rnns[0]._W0 = nn.Parameter(model.rnns[0]._W0.detach().clone())\n",
    "    ws_model.rnns[0].initialize_ws_by_values(\n",
    "        init_tensors=[p.detach() for p in model.rnns[0]._Ws]\n",
    "    )\n",
    "    ws_model.encoder.weight = nn.Parameter(model.encoder.weight.detach().clone())\n",
    "    ws_model.decoder.weight = ws_model.encoder.weight\n",
    "    ws_model.decoder.bias = nn.Parameter(model.decoder.bias.detach().clone())\n",
    "    return ws_model\n",
    "\n",
    "ws_model = initialize_soft_ws_from_original(model, ws_model)\n",
    "\n",
    "ws_model_params = ws_model.rnns[0].compute_ws()\n",
    "model_params = [p for p in model.rnns[0].parameters()]\n",
    "\n",
    "# CHeck the size and value. 0 is indeed.\n",
    "print(len(ws_model_params), len(model_params))\n",
    "print([w.size() for w in ws_model_params])\n",
    "print([w.size() for w in model.rnns[0]._Ws])\n",
    "print((model_params[1] - ws_model_params[0]).sum())\n",
    "print((ws_model.rnns[0]._W0 - model.rnns[0]._W0).sum())\n",
    "\n",
    "# Prepare the data.\n",
    "from dataloader import Corpus\n",
    "import utils \n",
    "corpus = Corpus('data/penn')\n",
    "args.cuda = 1\n",
    "train_data = utils.batchify(corpus.train, batch_size, args)\n",
    "model.cuda()\n",
    "ws_model.cuda()\n",
    "hidden = model.init_hidden(batch_size)\n",
    "\n",
    "# Testing forward pass\n",
    "inp_x, inp_y = utils.get_batch(train_data, 10, args, 35)\n",
    "output_1 = model(inp_x, hidden, return_h=True)\n",
    "output_2 = ws_model(inp_x, hidden, return_h=True)\n",
    "\n",
    "print((output_1[0]- output_2[0]).sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "before optimization:  tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\nparam 1 difference 0.0\nparam 2 difference 0.0\nparam 0 difference 0.0\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210322380065918\nLoss of new model :  9.210322380065918\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210332870483398\nLoss of new model :  9.210332870483398\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210433959960938\nLoss of new model :  9.210433959960938\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210362434387207\nLoss of new model :  9.210362434387207\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 4.656612873077393e-10\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.230583190917969\nLoss of new model :  9.230583190917969\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210433006286621\nLoss of new model :  9.210433006286621\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.277362823486328\nLoss of new model :  9.277362823486328\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210456848144531\nLoss of new model :  9.210456848144531\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 7.450580596923828e-09\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210479736328125\nLoss of new model :  9.210479736328125\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 1.4901161193847656e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.23274040222168\nLoss of new model :  9.23274040222168\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: -1.4901161193847656e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.21042251586914\nLoss of new model :  9.21042251586914\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210433959960938\nLoss of new model :  9.210433959960938\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: -1.4901161193847656e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.252991676330566\nLoss of new model :  9.252991676330566\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.21048355102539\nLoss of new model :  9.21048355102539\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.2435302734375\nLoss of new model :  9.2435302734375\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210326194763184\nLoss of new model :  9.210326194763184\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.275318145751953\nLoss of new model :  9.275318145751953\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: -5.960464477539063e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210347175598145\nLoss of new model :  9.210347175598145\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 2.9802322387695312e-08\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210412979125977\nLoss of new model :  9.210412979125977\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210395812988281\nLoss of new model :  9.210395812988281\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: -5.960464477539063e-08\nW2 grad compare: -2.9802322387695312e-08\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210282325744629\nLoss of new model :  9.210282325744629\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210333824157715\nLoss of new model :  9.210333824157715\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: -2.9802322387695312e-08\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.254934310913086\nLoss of new model :  9.254934310913086\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: -2.9802322387695312e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210390090942383\nLoss of new model :  9.210390090942383\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: -5.960464477539063e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210366249084473\nLoss of new model :  9.210366249084473\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 2.9802322387695312e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.269050598144531\nLoss of new model :  9.269050598144531\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.234335899353027\nLoss of new model :  9.234335899353027\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 5.960464477539063e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210505485534668\nLoss of new model :  9.210505485534668\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210463523864746\nLoss of new model :  9.210463523864746\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.210474967956543\nLoss of new model :  9.210474967956543\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: -5.960464477539063e-08\nW2 grad compare: 0.0\nFinish batch \n\nStart a new batch ... \nModel output output 1 - output 2 =  0.0\nLoss of original model :  9.231800079345703\nLoss of new model :  9.231800079345703\n",
      "Encoder grad compare: 0.0\nDecoder grad compare: 0.0\nW0 grad compare: 0.0\nW1 grad compare: 0.0\nW2 grad compare: 0.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Utility functions\n",
    "def compare_gradient_ws_model(model, ws_model):\n",
    "    value_pair = {\n",
    "        'Encoder': [model.encoder.weight, ws_model.encoder.weight],\n",
    "        'Decoder': [model.decoder.weight ,ws_model.decoder.weight],\n",
    "        'W0' : [model.rnns[0]._W0, ws_model.rnns[0]._W0],\n",
    "        'W1' : [model.rnns[0]._Ws[0], ws_model.rnns[0]._Ws[0]],\n",
    "        'W2' : [model.rnns[0]._Ws[1], ws_model.rnns[0]._Ws[4]],\n",
    "    }\n",
    "    for k, v in value_pair.items():\n",
    "        grads = [p.grad.norm().item() if p.grad is not None else None for p in v]\n",
    "        if k in ['W1', 'W2']:\n",
    "            grads[1] = 4 * grads[1]\n",
    "        if all(grads):\n",
    "            print(f\"{k} grad compare: {grads[0] - grads[1]}\")\n",
    "        else:\n",
    "            print(f\"{k} grad compare: {grads[0]} | ws {grads[1]}\")\n",
    "\n",
    "def compare_weights_ws_model(model, ws_model):\n",
    "    # Compare the param difference\n",
    "    ws_model_params = ws_model.rnns[0].compute_ws()\n",
    "    model_params = [p for p in model.rnns[0].parameters()]\n",
    "    print(\"param 1 difference\", utils.to_item((model_params[1] - ws_model_params[0]).sum()))\n",
    "    print(\"param 2 difference\", utils.to_item((model_params[2] - ws_model_params[1]).sum()))\n",
    "    print(\"param 0 difference\",(ws_model.rnns[0]._W0 - model.rnns[0]._W0).sum().item())\n",
    "\n",
    "\n",
    "def compare_raw_weights(model, ws_model):\n",
    "    ws = ws_model.rnns[0].compute_ws()\n",
    "    value_pair = {\n",
    "        'Encoder': [model.encoder.weight, ws_model.encoder.weight],\n",
    "        'Decoder': [model.decoder.weight ,ws_model.decoder.weight],\n",
    "        'W0' : [model.rnns[0]._W0, ws_model.rnns[0]._W0],\n",
    "        'W1' : [model.rnns[0]._Ws[0], ws_model.rnns[0]._Ws[0]],\n",
    "        'W2' : [model.rnns[0]._Ws[1], ws_model.rnns[0]._Ws[4]],\n",
    "        'W1-compute' : [model.rnns[0]._Ws[0], ws[0]],\n",
    "        'W2-compute' : [model.rnns[0]._Ws[1], ws[1]],\n",
    "    }\n",
    "    for k, v in value_pair.items():\n",
    "        weights = [w.norm().item() for w in v]\n",
    "        if all(weights):\n",
    "            print(f\"{k} weights norm compare : {weights[0]} | {weights[1]}\")\n",
    "\n",
    "# testing the backward and forward in 10 batches\n",
    "import numpy as np\n",
    "from torch.optim import SGD\n",
    "batch, i, pop_index = 0, 0, 0\n",
    "\n",
    "hidden = model.init_hidden(batch_size)\n",
    "output_1 = model(inp_x, hidden, return_h=True)\n",
    "output_2 = ws_model(inp_x, hidden, return_h=True)\n",
    "print('before optimization: ', (output_2[0] - output_1[0]).sum())\n",
    "compare_weights_ws_model(model, ws_model)\n",
    "\n",
    "optimizer1 = SGD([model.rnns[0]._W0], lr=0.01)\n",
    "# optimizer1 = SGD(model.parameters(), lr=0.01)\n",
    "optimizer2 = SGD([ws_model.rnns[0]._W0], lr=0.01)\n",
    "# optimizer2 = SGD(ws_model.parameters(), lr=0.01)\n",
    "# \n",
    "# Looping the dataset.\n",
    "while i < train_data.size(0) - 1 - 1:\n",
    "    print(\"Start a new batch ... \")\n",
    "    # computing the genotype of the next particle\n",
    "    genotype_id = np.random.randint(32)\n",
    "    new_genotype = search_space.genotype_from_id(genotype_id)\n",
    "\n",
    "    # selecting the current subDAG in our DAG to train\n",
    "    model.change_genotype(genotype=new_genotype, genotype_id=genotype_id)\n",
    "    ws_model.change_genotype(genotype=new_genotype, genotype_id=genotype_id)\n",
    "\n",
    "    bptt = 35\n",
    "    seq_len = int(bptt)\n",
    "\n",
    "    # preparing batch of data for training\n",
    "    cur_data, cur_targets = utils.get_batch(train_data, i, args, seq_len=seq_len)\n",
    "    cur_targets = cur_targets.contiguous().view(-1)\n",
    "    # print(cur_data.size(2))\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    \n",
    "    hidden = utils.repackage_hidden(hidden)\n",
    "    # hidden_valid[s_id] = repackage_hidden(hidden_valid[s_id])\n",
    "\n",
    "    # forward pass\n",
    "    log_prob, _, rnn_hs, dropped_rnn_hs = model(cur_data,hidden,return_h=True)\n",
    "    # print('hidden norm before', hidden[0].norm())\n",
    "    hidden = utils.repackage_hidden(hidden)\n",
    "    log_prob2, _, rnn_hs2, dropped_rnn_hs2 = ws_model(cur_data,hidden,return_h=True)\n",
    "    print(\"Model output output 1 - output 2 = \", (log_prob - log_prob2).sum().item())\n",
    "    # print('hidden norm after', hidden[0].norm())\n",
    "    \n",
    "    # loss using negative-log-likelihood\n",
    "    raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "    raw_loss2 = nn.functional.nll_loss(log_prob2.view(-1, log_prob2.size(2)), cur_targets)\n",
    "    \n",
    "    loss = raw_loss\n",
    "    loss2 = raw_loss2\n",
    "    print(\"Loss of original model : \", loss.item())\n",
    "    print(\"Loss of new model : \", loss2.item())\n",
    "    \n",
    "    loss2.backward()\n",
    "    loss.backward()\n",
    "\n",
    "    # applying the gradient updates\n",
    "    # utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "    # utils.clip_grad_norm(ws_model.parameters(), args.clip)\n",
    "    \n",
    "    compare_gradient_ws_model(model, ws_model)\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    \n",
    "    batch += 1\n",
    "    i += seq_len\n",
    "    if batch > 30:\n",
    "        break\n",
    "        \n",
    "    print(\"Finish batch \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Encoder weights norm compare : 67.33435821533203 | 67.33435821533203\nDecoder weights norm compare : 67.33435821533203 | 67.33435821533203\nW0 weights norm compare : 39.251556396484375 | 39.251556396484375\nW1 weights norm compare : 27.766395568847656 | 27.766395568847656\nW2 weights norm compare : 27.747833251953125 | 27.747833251953125\nW1-compute weights norm compare : 27.766395568847656 | 27.766395568847656\nW2-compute weights norm compare : 27.747833251953125 | 27.747833251953125\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "compare_raw_weights(model, ws_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Before step\nparam 1 difference 0.0\nparam 2 difference 0.0\nparam 0 difference -0.6326582431793213\nAfter step 1\nparam 1 difference 0.0\nparam 2 difference 0.0\nparam 0 difference -0.6183065176010132\nAfter step 2\nparam 1 difference 0.0\nparam 2 difference 0.0\nparam 0 difference -0.6303144097328186\n11\n5\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Before step\")\n",
    "compare_weights_ws_model(model, ws_model)\n",
    "optimizer1.step()\n",
    "print(\"After step 1\")\n",
    "compare_weights_ws_model(model, ws_model)\n",
    "optimizer2.step()\n",
    "print(\"After step 2\")\n",
    "compare_weights_ws_model(model, ws_model)\n",
    "\n",
    "\n",
    "# check if the weights of ws model are the same or not.\n",
    "\n",
    "# print(ws_model.rnns[0].soft_param_dicts[0][0])\n",
    "print(len([p.size() for p in ws_model.parameters()]))\n",
    "print(len([p.size() for p in model.parameters()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "RNNModelSoftWS(\n  (lockdrop): LockedDropout()\n  (encoder): Embedding(10000, 850)\n  (rnns): ModuleList(\n    (0): BenchmarkCellSoftWS(\n      (_Ws): ParameterList(\n          (0): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n          (1): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n          (2): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n          (3): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n          (4): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n          (5): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n          (6): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n          (7): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n      )\n    )\n  )\n  (decoder): Linear(in_features=850, out_features=10000, bias=True)\n)\nRNNModel(\n  (lockdrop): LockedDropout()\n  (encoder): Embedding(10000, 850)\n  (rnns): ModuleList(\n    (0): BenchmarkCell(\n      (_Ws): ParameterList(\n          (0): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n          (1): Parameter containing: [torch.cuda.FloatTensor of size 850x1700 (GPU 0)]\n      )\n    )\n  )\n  (decoder): Linear(in_features=850, out_features=10000, bias=True)\n)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(ws_model)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[torch.Size([10000, 850]), torch.Size([1700, 1700]), torch.Size([850, 1700]), torch.Size([850, 1700]), torch.Size([10000])]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print([p.grad.size() for p in model.parameters()])\n",
    "# print([p.grad.size() for p in ws_model.parameters()])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[torch.Size([10000, 850]), torch.Size([1700, 1700]), torch.Size([850, 1700]), torch.Size([850, 1700]), torch.Size([850, 1700]), torch.Size([850, 1700]), torch.Size([850, 1700]), torch.Size([850, 1700]), torch.Size([850, 1700]), torch.Size([850, 1700]), torch.Size([10000])]\ntensor([[-2.3829e-05, -1.3522e-05,  2.8911e-05,  ..., -1.2082e-04,\n         -1.2098e-04,  1.7041e-04],\n        [-2.2591e-05, -1.4650e-05,  2.8086e-05,  ..., -1.4077e-04,\n         -1.5443e-04,  2.2530e-04],\n        [-3.2155e-05, -1.8940e-05,  3.7770e-05,  ..., -1.6261e-04,\n         -1.7970e-04,  2.4731e-04],\n        ...,\n        [-1.2547e-05, -6.9961e-06,  1.4064e-05,  ..., -7.7625e-05,\n         -7.0915e-05,  9.7396e-05],\n        [-2.9369e-05, -1.7936e-05,  3.6615e-05,  ..., -1.6770e-04,\n         -1.7058e-04,  2.3913e-04],\n        [ 8.8107e-06,  5.8585e-06, -8.3779e-06,  ...,  7.0062e-05,\n          7.8304e-05, -1.1476e-04]], device='cuda:0')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "# loss2.backward()\n",
    "print([p.grad.size() for p in ws_model.parameters()])\n",
    "# optimizer1.step()\n",
    "print(ws_model.rnns[0]._Ws[0].grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing the mapping function.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Genotype 0 Genotype(recurrent=[('tanh', 0), ('tanh', 0)], concat=range(1, 3))\n[[0.85, 0.05, 0.05, 0.05], [0.85, 0.05, 0.05, 0.05]]\nGenotype 1 Genotype(recurrent=[('tanh', 0), ('relu', 0)], concat=range(1, 3))\n[[0.05, 0.85, 0.05, 0.05], [0.85, 0.05, 0.05, 0.05]]\nGenotype 2 Genotype(recurrent=[('tanh', 0), ('sigmoid', 0)], concat=range(1, 3))\n[[0.05, 0.05, 0.85, 0.05], [0.85, 0.05, 0.05, 0.05]]\nGenotype 3 Genotype(recurrent=[('tanh', 0), ('identity', 0)], concat=range(1, 3))\n[[0.05, 0.05, 0.05, 0.85], [0.85, 0.05, 0.05, 0.05]]\nGenotype 4 Genotype(recurrent=[('tanh', 0), ('tanh', 1)], concat=range(1, 3))\n[[0.85, 0.05, 0.05, 0.05], [0.85, 0.05, 0.05, 0.05]]\nGenotype 5 Genotype(recurrent=[('tanh', 0), ('relu', 1)], concat=range(1, 3))\n[[0.05, 0.85, 0.05, 0.05], [0.85, 0.05, 0.05, 0.05]]\nGenotype 6 Genotype(recurrent=[('tanh', 0), ('sigmoid', 1)], concat=range(1, 3))\n[[0.05, 0.05, 0.85, 0.05], [0.85, 0.05, 0.05, 0.05]]\nGenotype 7 Genotype(recurrent=[('tanh', 0), ('identity', 1)], concat=range(1, 3))\n[[0.05, 0.05, 0.05, 0.85], [0.85, 0.05, 0.05, 0.05]]\nGenotype 8 Genotype(recurrent=[('relu', 0), ('tanh', 0)], concat=range(1, 3))\n[[0.85, 0.05, 0.05, 0.05], [0.05, 0.85, 0.05, 0.05]]\nGenotype 9 Genotype(recurrent=[('relu', 0), ('relu', 0)], concat=range(1, 3))\n[[0.05, 0.85, 0.05, 0.05], [0.05, 0.85, 0.05, 0.05]]\nGenotype 10 Genotype(recurrent=[('relu', 0), ('sigmoid', 0)], concat=range(1, 3))\n[[0.05, 0.05, 0.85, 0.05], [0.05, 0.85, 0.05, 0.05]]\nGenotype 11 Genotype(recurrent=[('relu', 0), ('identity', 0)], concat=range(1, 3))\n[[0.05, 0.05, 0.05, 0.85], [0.05, 0.85, 0.05, 0.05]]\nGenotype 12 Genotype(recurrent=[('relu', 0), ('tanh', 1)], concat=range(1, 3))\n[[0.85, 0.05, 0.05, 0.05], [0.05, 0.85, 0.05, 0.05]]\nGenotype 13 Genotype(recurrent=[('relu', 0), ('relu', 1)], concat=range(1, 3))\n[[0.05, 0.85, 0.05, 0.05], [0.05, 0.85, 0.05, 0.05]]\nGenotype 14 Genotype(recurrent=[('relu', 0), ('sigmoid', 1)], concat=range(1, 3))\n[[0.05, 0.05, 0.85, 0.05], [0.05, 0.85, 0.05, 0.05]]\nGenotype 15 Genotype(recurrent=[('relu', 0), ('identity', 1)], concat=range(1, 3))\n[[0.05, 0.05, 0.05, 0.85], [0.05, 0.85, 0.05, 0.05]]\nGenotype 16 Genotype(recurrent=[('sigmoid', 0), ('tanh', 0)], concat=range(1, 3))\n[[0.85, 0.05, 0.05, 0.05], [0.05, 0.05, 0.85, 0.05]]\nGenotype 17 Genotype(recurrent=[('sigmoid', 0), ('relu', 0)], concat=range(1, 3))\n[[0.05, 0.85, 0.05, 0.05], [0.05, 0.05, 0.85, 0.05]]\nGenotype 18 Genotype(recurrent=[('sigmoid', 0), ('sigmoid', 0)], concat=range(1, 3))\n[[0.05, 0.05, 0.85, 0.05], [0.05, 0.05, 0.85, 0.05]]\nGenotype 19 Genotype(recurrent=[('sigmoid', 0), ('identity', 0)], concat=range(1, 3))\n[[0.05, 0.05, 0.05, 0.85], [0.05, 0.05, 0.85, 0.05]]\nGenotype 20 Genotype(recurrent=[('sigmoid', 0), ('tanh', 1)], concat=range(1, 3))\n[[0.85, 0.05, 0.05, 0.05], [0.05, 0.05, 0.85, 0.05]]\nGenotype 21 Genotype(recurrent=[('sigmoid', 0), ('relu', 1)], concat=range(1, 3))\n[[0.05, 0.85, 0.05, 0.05], [0.05, 0.05, 0.85, 0.05]]\nGenotype 22 Genotype(recurrent=[('sigmoid', 0), ('sigmoid', 1)], concat=range(1, 3))\n[[0.05, 0.05, 0.85, 0.05], [0.05, 0.05, 0.85, 0.05]]\nGenotype 23 Genotype(recurrent=[('sigmoid', 0), ('identity', 1)], concat=range(1, 3))\n[[0.05, 0.05, 0.05, 0.85], [0.05, 0.05, 0.85, 0.05]]\nGenotype 24 Genotype(recurrent=[('identity', 0), ('tanh', 0)], concat=range(1, 3))\n[[0.85, 0.05, 0.05, 0.05], [0.05, 0.05, 0.05, 0.85]]\nGenotype 25 Genotype(recurrent=[('identity', 0), ('relu', 0)], concat=range(1, 3))\n[[0.05, 0.85, 0.05, 0.05], [0.05, 0.05, 0.05, 0.85]]\nGenotype 26 Genotype(recurrent=[('identity', 0), ('sigmoid', 0)], concat=range(1, 3))\n[[0.05, 0.05, 0.85, 0.05], [0.05, 0.05, 0.05, 0.85]]\nGenotype 27 Genotype(recurrent=[('identity', 0), ('identity', 0)], concat=range(1, 3))\n[[0.05, 0.05, 0.05, 0.85], [0.05, 0.05, 0.05, 0.85]]\nGenotype 28 Genotype(recurrent=[('identity', 0), ('tanh', 1)], concat=range(1, 3))\n[[0.85, 0.05, 0.05, 0.05], [0.05, 0.05, 0.05, 0.85]]\nGenotype 29 Genotype(recurrent=[('identity', 0), ('relu', 1)], concat=range(1, 3))\n[[0.05, 0.85, 0.05, 0.05], [0.05, 0.05, 0.05, 0.85]]\nGenotype 30 Genotype(recurrent=[('identity', 0), ('sigmoid', 1)], concat=range(1, 3))\n[[0.05, 0.05, 0.85, 0.05], [0.05, 0.05, 0.05, 0.85]]\nGenotype 31 Genotype(recurrent=[('identity', 0), ('identity', 1)], concat=range(1, 3))\n[[0.05, 0.05, 0.05, 0.85], [0.05, 0.05, 0.05, 0.85]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "import search_policies.rnn.softws.soft_ws_mapping_node2 as genotype_mapping \n",
    "from search_policies.rnn.softws.soft_weight_sharing_model import get_fn_map\n",
    "\n",
    "def visualize_mapping_for_genotype(map_fn, search_space):\n",
    "    # if issubclass(map_fn, genotype_mapping.StoreQueryFull):\n",
    "    #     map_fn = map_fn([4, 4], 2, search_space=search_space)\n",
    "    for i in range(32):        \n",
    "        g_id = i\n",
    "        genotype = search_space.genotype_from_id(i)\n",
    "        print(\"Genotype {} {}\".format(g_id, genotype))\n",
    "        print(map_fn(genotype, g_id, [4, 4], 2))\n",
    "        \n",
    "\n",
    "\n",
    "# visualize_mapping_for_genotype(genotype_mapping.map_v1, search_space)\n",
    "# visualize_mapping_for_genotype(genotype_mapping.map_v2, search_space)\n",
    "# visualize_mapping_for_genotype(get_fn_map('soft_map_v3', None), search_space)\n",
    "# visualize_mapping_for_genotype(genotype_mapping.map_random_v1, search_space)\n",
    "# visualize_mapping_for_genotype(genotype_mapping.map_random_v2, search_space)\n",
    "# visualize_mapping_for_genotype(genotype_mapping.map_random_v3, search_space)\n",
    "# visualize_mapping_for_genotype(default_genotype_to_param_query, search_space)\n",
    "args = Namespace(softws_init_v=0.05)\n",
    "visualize_mapping_for_genotype(get_fn_map('soft_map_v3_init', args), search_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "map_fn = genotype_mapping.map_random_v2([4,4], 2, search_space=search_space)\n",
    "print(map_fn.id_query)\n",
    "print(search_space.num_solutions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pytorch1.0",
   "language": "python",
   "display_name": "pytorch-latest"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}